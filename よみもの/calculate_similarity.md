<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/MathJax.js?config=TeX-MML-AM_CHTML">
</script>
<script type="text/x-mathjax-config">
 MathJax.Hub.Config({
 tex2jax: {
 inlineMath: [['$', '$'] ],
 displayMath: [ ['$$','$$'], ["\\[","\\]"] ]
 }
 });
</script>

# Calculate Similarity


（特に細かい文脈とかは考えずに）文章の類似度を計算します  
本題は[1. TF-IDFによるベクトル化と類似度計算](#1-tf-idfによるベクトル化と類似度計算)以降からどうぞ

- [Calculate Similarity](#calculate-similarity)
  - [Intro - How calculate the similarity?](#intro---how-calculate-the-similarity)
    - [類似度の話をする前に……](#類似度の話をする前に)
    - [1. 文を単語に区切る](#1-文を単語に区切る)
    - [2. 単語・文章を数値で表す](#2-単語文章を数値で表す)
    - [3. 類似度を計算する](#3-類似度を計算する)
  - [1. TF-IDFによるベクトル化と類似度計算](#1-tf-idfによるベクトル化と類似度計算)
      - [Practice](#practice)
      - [Advantage & Disadvantage](#advantage--disadvantage)
  - [2. Word Embeddingによるベクトル化と類似度計算](#2-word-embeddingによるベクトル化と類似度計算)
      - [Practice](#practice-1)
      - [Advantage & Disadvantage](#advantage--disadvantage-1)
  - [しょかん](#しょかん)
- [References](#references)


## Intro - How calculate the similarity?

### 類似度の話をする前に……

SiriやAlexaに「音楽を流して」と話しかけたとき、



コンピュータに文章を理解させるには、以下のプロセスがよく使用されています。

1. 文を単語に区切る  
英語のリスニングなどで、全てを聞き取れなくとも重要な単語さえ聞き取りその意味を知っていたのでなんとなく話の内容が予測できたという経験があると思います。  
文を理解するには、その最小単位である単語の意味を理解することが重要であり、その意味は**他にどの単語と共に使われているか**から推測することになります。  

2. 単語・文章を数値で表す  
計算機は基本的に数値しか取り扱えないためです。これによって（擬似的な）意味を数値を用いて客観的に表現します。これによく使われる手法には、後半で取り上げる`TF-IDF`や`Word Embedding`があります。

3. 類似度を計算する

以上はよく見かけるとは言え、数ある類似度の計算方法のうちの一部に過ぎません。
他の計算方法については、[References](#references)に挙げる『テキスト処理の要素技術』や『はじめての自然言語処理』をお読みください。


### 1. 文を単語に区切る
単語の区切りがスペースで行われる英語と違って、日本語ではどこからどこまでを一単語として扱えばよいかがわかりにくくなっています。それゆえ、日本語の自然言語処理においては単語の分割が重要になってきます。  
日本語の文章で単語を区切るのによく利用されている手法は以下の２つです。

- `N-gram` : 文を先頭からN文字ずつ区切る
- `形態素解析` : 文法知識を集約した辞書を用いて文を形態素（意味を持つ最小の単語≒品詞）に分ける


「音楽を流して」という文をこの２つの方法で分けてみます。  
日本語では多くの場合、N-gramのうちのN=2のBigramやN=3のTrigramで処理されることが多いそうなので、今回はN=2,3で分割します。確かに言われてみれば日常使われているのは２〜３語の熟語がほとんどですね。


|Method|Result|
|------|-------|
|N-gram(N=2)|音楽 / 楽を / を流 / 流し / して|
|N-gram(N=3)|音楽を / 楽を流 / を流し / 流して|
|形態素解析|音楽 / を / 流し / て|

一見、形態素解析を用いた分割のほうが、私たちの直感に沿っており、意味の通じない単語の出現がありません。


<!--なんかここなくてもいいかな>

「今日本は夜の九時です」

この例文を日本語を習得した主体が読めば、「今/日本/は/夜/の/九時/です」と分けられます。しかし、単語間の関係を十分には解せないコンピュータにとっては「今日/本/は/夜/の/九時/です」と分けることもできます。

<!-->

>「彼は米原発の電車に乗ったそうだ」  [引用元: Twitter](https://twitter.com/hiroshima_pot/status/1471076670711169026?s=21)

一方でこの例文では、「米原（まいばら）」という地名を知っていれば（＝辞書に単語があれば）`「彼/は/米原/発/の/電車/に/乗っ/た/そうだ」`とすんなり分けることができますが、そうでなければ`「彼/は/米/原発/の/電車/に/乗っ/た/そうだ」`と分けてしまうことも有り得るでしょう。後者を英訳すると「the U.S. nuclear power plant（アメリカの原子力発電所）」という、当初の文で想定されなかったフレーズが突然現れてしまいます。  
このケースではBigramを使用すれば`「彼は/は米/米原/原発/発の/の電/電車/に乗/乗っ/った/たそ/そう/うだ」`とノイズが混じってしまうものの「米原」を取り出せます。形態素解析に用いる辞書にない単語（＝`未知語`）を見落としたくない場合には、N-gramが効果的なこともあります。

ここまでの内容を踏まえると以下のことがわかります。

- 文章を単語間の関係を考慮した上で区切るのは難しい
- 形態素解析を用いる場合には、文中に出現する単語をできる限り網羅した辞書が必要だが、そのぶん結果は人が見て違和感がないことが多い
- N-gramを用いると、結果にノイズが含まれてしまうが、辞書にない語も拾えることがある

……若干の不安が残る結論ですね。
どんな文章でも完璧に区切るのは不可能ですが、N-gramと形態素解析の両方を使用したり、対象となる分野の単語辞書を利用・作成すれば、ある程度の水準に到達することは多いみたいです。  

とはいえ形態素解析ツールには`MeCab`、`JUMAN++`、`Sudachi`など手軽に利用できるものがあり、定期的に語句が追加されている辞書や、分野ごとの辞書も公開されているため、基本的には形態素解析を用いて分割するのが無難だと思います。


### 2. 単語・文章を数値で表す

単語や文章を数値で表す（＝`ベクトル化`）には、後述する`TF-IDF`や`Word Embedding`をはじめとしてさまざまな手法がありますが、ここでは出現単語の種類数と出現頻度によって表す方法で説明します。

「ネコは食肉目ネコ科の動物です」と「ネコグサはキンポウゲ科の多年草です」という２つの例文を使って考えます。

まずは全ての文書から単語を取り出し、重複のないボキャブラリー集合を作成します。

```
vocaburary：{ネコ, ネコグサ, 食肉, 目, キンポウゲ, 科, 動物, 多年, 草, は, の, です}
```

ボキャブラリーは全部で12個あるので、一つの単語は12次元のベクトルで表すことができます。ボキャブラリーに出てくる順番を行列で示すことで、単語を数値に置き換えられます。  
数値に置き換えられた単語を一部を見てみると以下のようになります。

```
ネコ: {1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}
ネコグサ: {0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0}
です: {0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1}
```

この単語ベクトルと各文書での出現回数を使って、文章をベクトル化すると以下のようになります。

|         vocaburary:            |ネコ|ネコグサ|食肉|目|キンポウゲ|科|動物|多年|草|は|の|です|
|--------------------------------|----|--------|----|--|----------|--|----|----|--|--|--|----|
|「ネコは食肉目ネコ科の動物です」  |2|0|1|1|0|1|1|0|0|1|1|1|
|「ネコグサはキンポウゲ科の多年草です」|0|1|0|0|1|1|0|1|1|1|1|1|


また、このボキャブラリーを使って新しい文章「パンダは食肉目パンダ科の動物です」をベクトルに直してみます。

|         vocaburary:            |ネコ|ネコグサ|食肉|目|キンポウゲ|科|動物|多年|草|は|の|です|
|--------------------------------|----|--------|----|--|----------|--|------|----|--|--|--|----|
|「パンダは食肉目パンダ科の動物です」|0|0|1|1|0|1|1|0|0|1|1|1|


ボキャブラリーにない単語である「パンダ」はベクトル化の際に無視されてしまいます。正確にベクトル化するには、できるだけ多くの単語を含んだボキャブラリーが必要になってくるということですね。

ちなみに、基本的に文書数が少なければ有用なベクトルを得ることができません。  
少ない文書数で文書のベクトル化を行うには、事前に多数の文書で学習された単語ベクトルモデルを使用しましょう。

### 3. 類似度を計算する

類似度の計算にもさまざまな手法があり、問題設定によって選択すべきですが、`コサイン類似度`が利用される場面が多いように感じます。  

[コサイン類似度 | 高校数学の美しい物語](https://manabitimes.jp/math/1378)

ベクトルの向きを文書の意味とし、ベクトル間の角度を類似度とみなします。~~は？と思ってもそういうものなのかと流してください~~

コサイン類似度のメリットは、**文章の長さに影響されず、単語の意味による類似度を計算できる**ところにあります。

コサイン類似度は下記の公式によって求められます。

$\vec{a}=(a_1,a_2,a_3,…,a_n)$ と $\vec{b}=(b_1,b_2,b_3,…,b_n)$ の２つのベクトルについて
$$ \cos{(\vec{a}, \vec{b})} =  \frac{\vec{a}\cdot\vec{b}}{\|{\vec{a}}\|\|\vec{b}\|} = \frac{\displaystyle\sum_{i=1}^na_ib_i}{\sqrt{\displaystyle\sum_{i=1}^n}a_i^2 \sqrt{\displaystyle\sum_{i=1}^n}b_i^2} $$

（内積$\vec{a}\cdot\vec{b}$について、成分より$\vec{a}\cdot\vec{b}=a_1b_1+a_2b_2…$、ベクトルより$\vec{a}\cdot\vec{b}=|\vec{a}||\vec{b}|\cos{\theta}=\sqrt{a_1^2+a_2^2…}\sqrt{b_1^2+b_2…}\cos{\theta}$ と表せるので、 $a_1b_1+a_2b_2… =\sqrt{a_1^2+a_2^2…}\sqrt{b_1^2+b_2…}\cos{\theta}$とおくことができ、この方程式を$\cos{\theta}$について解けばいいですね、という話らしいです。二直線のなす角の公式のことらしいです。）


……原理の理解は一旦置いといて、Pythonでは下記のコードを書いて求められます。

```
import numpy as np

def cosine_similarity(vec1, vec2):
  return np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))
```

先程求めた文書ベクトルを元にコサイン類似度を求めてみましょう。

```
# np.array(配列)でnumpy型に型変換しています
text_neko = np.array([2,0,1,1,0,1,1,0,0,1,1,1])
text_nekogusa = np.array([0,1,0,0,1,1,0,1,1,1,1,1])
text_panda = np.array([0,0,1,1,0,1,1,0,0,1,1,1])

print('text_neko vs text_nekogusa :', cosine_similarity(text_neko, text_nekogusa))
print('text_neko vs text_panda :', cosine_similarity(text_neko, text_panda))
print('text_nekogusa vs text_panda :', cosine_similarity(text_nekogusa, text_panda))
```

```
# 出力
text_neko vs text_nekogusa : 0.42640143271122083
text_neko vs text_panda : 0.7977240352174656
text_nekogusa vs text_panda : 0.5345224838248487
```

コサイン類似度は`-1から1`までの範囲をとり、`1`に近いほど類似していると判定されます。

今回の例で考えると、「ネコは食肉目ネコ科の動物です」と「パンダは食肉目パンダ科の動物です」が最も類似しており、「ネコは食肉目ネコ科の動物です」と「ネコグサはキンポウゲ科の多年草です」が類似していないという結果になりました。  
「食肉目」が共通して登場する前者の組み合わせが近似しているというのは妥当な判断そうです。

簡単ではありますが、単語の分割・単語のベクトル化・コサイン類似度まで確認しました。  
次からはTF-IDF・Word Embeddingを用いたベクトル化と、類似度の計算について見ていきます。

---

## 1. TF-IDFによるベクトル化と類似度計算
`TF-IDF`は、ある文書における、ある単語の重要度を計算するのに用いられる値です。
- `TF(単語頻度/Term Frequency)` : ある文書の中で、ある単語が出現する頻度
- `IDF(逆文書頻度/Inverse Document Frequency)` : ある単語が含まれる文書数の割合の逆数
- `TF-IDF` : 

何を言ってるのかわからなくなってきた。

要するに、TF-IDFを用いて類似度の計算を行う場合、以下が行われるというわけです。

```

```

Pythonでは`scikit-learn`にTF-IDFを計算する関数があるので簡単に実装できます。


#### Practice

```
```

```
```

#### Advantage & Disadvantage

---

## 2. Word Embeddingによるベクトル化と類似度計算


#### Practice

#### Advantage & Disadvantage



---

## しょかん

[中国語の部屋 - Wikipedia](https://ja.wikipedia.org/wiki/%E4%B8%AD%E5%9B%BD%E8%AA%9E%E3%81%AE%E9%83%A8%E5%B1%8B)

マニュアルを数多く準備すれば客観的に意味を理解しているかのように見せることは可能であり、そのマニュアルやルールを作るために辞書の整備・様々な文書を用意することは必要不可欠

何をもって似ているかの判断は主観が入るような気がする
TF-IDFとかWord2vecとかBERTを使ってる気がする

---

# References
- 谷口忠大『イラストで学ぶ 人工知能概論』改訂第２版、講談社、2020年
- 山本和英『テキスト処理の要素技術 実践・自然言語処理シリーズ』近代科学社、2021年
- [はじめての自然言語処理 類似文書検索の手法と精度比較 | オブジェクトの広場](https://www.ogis-ri.co.jp/otc/hiroba/technical/similar-document-search/part1.html) (最終閲覧日:2022/1/12)
- [ユークリッド距離 vs コサイン類似度 | システム開発部Blog](https://enjoyworks.jp/tech-blog/2242) (最終閲覧日:2022/1/12)
- [5-11. レコメンドシステムではなぜユークリッド距離ではなくコサイン類似度が用いられるのか | Vignette & Clarity（ビネット＆クラリティ）](https://vigne-cla.com/5-11/) (最終閲覧日:2022/1/12)